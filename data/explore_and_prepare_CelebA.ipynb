{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image, ImageDraw\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from procrustes import procrustes\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import sys\n",
    "sys.path.append('../inference/')\n",
    "from face_detector import FaceDetector\n",
    "# this face detector is taken from here\n",
    "# https://github.com/TropComplique/FaceBoxes-tensorflow\n",
    "# (facial keypoints detector will be trained to work well with this detector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this script is to explore images/annotations of the CelebA dataset.  \n",
    "Also it cleans CelebA.  \n",
    "Also it converts annotations into json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_DIR = '/home/gpu2/hdd/dan/CelebA/img_celeba.7z/out/'\n",
    "ANNOTATIONS_PATH = '/home/gpu2/hdd/dan/CelebA/list_landmarks_celeba.txt'\n",
    "SPLIT_PATH = '/home/gpu2/hdd/dan/CelebA/list_eval_partition.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect paths to all images\n",
    "\n",
    "all_paths = []\n",
    "for name in tqdm(os.listdir(IMAGES_DIR)):\n",
    "    all_paths.append(os.path.join(IMAGES_DIR, name))\n",
    "\n",
    "metadata = pd.DataFrame(all_paths, columns=['full_path'])\n",
    "\n",
    "# strip root folder\n",
    "metadata['name'] = metadata.full_path.apply(lambda x: os.path.relpath(x, IMAGES_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of images is taken from the official website\n",
    "assert len(metadata) == 202599"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see all unique endings\n",
    "metadata.name.apply(lambda x: x.split('.')[-1]).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect a face on each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load faceboxes detector\n",
    "face_detector = FaceDetector('../inference/model-step-240000.pb', visible_device_list='0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = []\n",
    "for p in tqdm(metadata.full_path):\n",
    "    image = cv2.imread(p)\n",
    "    image = image[:, :, [2, 1, 0]]  # to RGB\n",
    "    detections.append(face_detector(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only images where one high confidence box is detected\n",
    "bad_images = [metadata.name[i] for i, (b, s) in enumerate(detections) if len(b) != 1 or s.max() < 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = {}\n",
    "for n, (box, score) in zip(metadata.name, detections):\n",
    "    if n not in bad_images:\n",
    "        ymin, xmin, ymax, xmax = box[0]\n",
    "        boxes[n] = (xmin, ymin, xmax, ymax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read keypoints from annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numbers(s):\n",
    "    s = s.strip().split(' ')\n",
    "    return [s[0]] + [int(i) for i in s[1:] if i]\n",
    "    \n",
    "with open(ANNOTATIONS_PATH, 'r') as f:\n",
    "    content = f.readlines()\n",
    "    content = content[2:]\n",
    "    content = [get_numbers(s) for s in content]\n",
    "\n",
    "landmarks = {}\n",
    "more_bad_images = []\n",
    "for i in content:\n",
    "    name = i[0]\n",
    "    \n",
    "    keypoints = [\n",
    "        [i[1], i[2]],  # lefteye_x lefteye_y \n",
    "        [i[3], i[4]],  # righteye_x righteye_y\n",
    "        [i[5], i[6]],  # nose_x nose_y \n",
    "        [i[7], i[8]],  # leftmouth_x leftmouth_y\n",
    "        [i[9], i[10]],  # rightmouth_x rightmouth_y\n",
    "    ]\n",
    "    \n",
    "    # assert that landmarks are inside the box\n",
    "    if name in bad_images:\n",
    "        continue\n",
    "    xmin, ymin, xmax, ymax = boxes[name]\n",
    "    points = np.array(keypoints)\n",
    "    is_normal = (points[:, 0] > xmin).all() and\\\n",
    "        (points[:, 0] < xmax).all() and\\\n",
    "        (points[:, 1] > ymin).all() and\\\n",
    "        (points[:, 1] < ymax).all()\n",
    "    if not is_normal:\n",
    "        more_bad_images.append(name)\n",
    "\n",
    "    landmarks[name] = keypoints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of weird landmarks\n",
    "len(more_bad_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = more_bad_images + bad_images\n",
    "metadata = metadata.loc[~metadata.name.isin(to_remove)]\n",
    "metadata = metadata.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup results\n",
    "metadata.to_csv('metadata.csv')\n",
    "np.save('boxes.npy', boxes)\n",
    "np.save('landmarks.npy', landmarks)\n",
    "np.save('to_remove.npy', to_remove)\n",
    "\n",
    "# metadata = pd.read_csv('metadata.csv', index_col=0)\n",
    "# boxes = np.load('boxes.npy')[()]\n",
    "# landmarks = np.load('landmarks.npy')[()]\n",
    "# to_remove = np.load('to_remove.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size after cleaning\n",
    "len(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show some bounding boxes and landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes_on_image(path, box, keypoints):\n",
    "\n",
    "    image = Image.open(path)\n",
    "    draw = ImageDraw.Draw(image, 'RGBA')\n",
    "\n",
    "    xmin, ymin, xmax, ymax = box\n",
    "    fill = (255, 255, 255, 45)\n",
    "    outline = 'red'\n",
    "    draw.rectangle(\n",
    "        [(xmin, ymin), (xmax, ymax)],\n",
    "        fill=fill, outline=outline\n",
    "    )\n",
    "    \n",
    "    for x, y in keypoints:\n",
    "        draw.ellipse([\n",
    "            (x - 2.0, y - 2.0),\n",
    "            (x + 2.0, y + 2.0)\n",
    "        ], outline='red')\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = random.randint(0, len(metadata) - 1)  # choose a random image\n",
    "some_boxes = boxes[metadata.name[i]]\n",
    "keypoints = landmarks[metadata.name[i]]\n",
    "draw_boxes_on_image(metadata.full_path[i], some_boxes, keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procrustes analysis (Pose-based Data Balancing strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_array = []\n",
    "boxes_array = []\n",
    "for n in metadata.name:\n",
    "    landmarks_array.append(np.array(landmarks[n]))\n",
    "    boxes_array.append(np.array(boxes[n]))\n",
    "\n",
    "landmarks_array = np.stack(landmarks_array, axis=0)\n",
    "landmarks_array = landmarks_array.astype('float32')\n",
    "boxes_array = np.stack(boxes_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_shape = landmarks_array.mean(0)  # reference shape\n",
    "num_images = len(landmarks_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned = []\n",
    "for shape in tqdm(landmarks_array):\n",
    "    Z, _ = procrustes(mean_shape, shape, reflection=False)\n",
    "    aligned.append(Z)\n",
    "aligned = np.stack(aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)\n",
    "projected = pca.fit_transform(aligned.reshape((-1, 10)))\n",
    "projected = projected[:, 0]\n",
    "\n",
    "plt.hist(projected, bins=40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frontal faces:\n",
    "indices = np.where(np.abs(projected) < 5)[0]\n",
    "\n",
    "# faces turned to the left:\n",
    "# indices = np.where(projected > 15)[0]\n",
    "\n",
    "# faces turned to the right:\n",
    "# indices = np.where(projected < -30)[0]\n",
    "\n",
    "i = indices[random.randint(0, len(indices) - 1)]\n",
    "some_boxes = boxes[metadata.name[i]]\n",
    "keypoints = landmarks[metadata.name[i]]\n",
    "draw_boxes_on_image(metadata.full_path[i], some_boxes, keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is not strictly a yaw angle\n",
    "metadata['yaw'] = projected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train-val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = pd.read_csv(SPLIT_PATH, header=None, sep=' ')\n",
    "split.columns = ['name', 'assignment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = split.loc[~split.name.isin(to_remove)]\n",
    "split = split.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split.assignment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"0\" represents training image, \"1\" represents validation image, \"2\" represents testing image\n",
    "train = list(split.loc[split.assignment.isin([0, 1]), 'name'])\n",
    "val = list(split.loc[split.assignment.isin([2]), 'name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upsample rare poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['is_train'] = metadata.name.isin(train).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [metadata.yaw.min() - 1.0, -20.0, -5.0, 5.0, 20.0, metadata.yaw.max() + 1.0]\n",
    "metadata['bin'] = pd.cut(metadata.yaw, bins, labels=False)\n",
    "metadata.loc[metadata.is_train == 1, 'bin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_to_upsample = [0, 1, 3, 4]\n",
    "num_samples = 80000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_metadata = metadata.loc[metadata.is_train == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled = [metadata.loc[(metadata.is_train == 1) & (metadata.bin == 2)]]\n",
    "for b in bins_to_upsample:\n",
    "    to_use = (metadata.is_train == 1) & (metadata.bin == b)\n",
    "    m = metadata.loc[to_use].sample(n=num_samples, replace=True)\n",
    "    upsampled.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled = pd.concat(upsampled)\n",
    "upsampled.bin.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.concat([upsampled, val_metadata])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotation(name, new_name, width, height, translation):\n",
    "    xmin, ymin, xmax, ymax = boxes[name]\n",
    "    keypoints = landmarks[name]\n",
    "    \n",
    "    tx, ty = translation\n",
    "    keypoints = [[p[0] - tx, p[1] - ty]for p in keypoints]\n",
    "    xmin, ymin = xmin - tx, ymin - ty\n",
    "    xmax, ymax = xmax - tx, ymax - ty\n",
    "    \n",
    "    annotation = {\n",
    "        \"filename\": new_name,\n",
    "        \"size\": {\"depth\": 3, \"width\": width, \"height\": height},\n",
    "        \"box\": {\"ymin\": int(ymin), \"ymax\": int(ymax), \"xmax\": int(xmax), \"xmin\": int(xmin)},\n",
    "        \"landmarks\": keypoints\n",
    "    }\n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folders for the converted dataset\n",
    "TRAIN_DIR = '/mnt/datasets/dan/CelebA/train/'\n",
    "shutil.rmtree(TRAIN_DIR, ignore_errors=True)\n",
    "os.mkdir(TRAIN_DIR)\n",
    "os.mkdir(os.path.join(TRAIN_DIR, 'images'))\n",
    "os.mkdir(os.path.join(TRAIN_DIR, 'annotations'))\n",
    "\n",
    "VAL_DIR = '/mnt/datasets/dan/CelebA/val/'\n",
    "shutil.rmtree(VAL_DIR, ignore_errors=True)\n",
    "os.mkdir(VAL_DIR)\n",
    "os.mkdir(os.path.join(VAL_DIR, 'images'))\n",
    "os.mkdir(os.path.join(VAL_DIR, 'annotations'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "for T in tqdm(metadata.itertuples()):\n",
    "    \n",
    "    # get width and height of an image\n",
    "    image = cv2.imread(T.full_path)\n",
    "    h, w, c = image.shape\n",
    "    assert c == 3\n",
    "    \n",
    "    # name of the image\n",
    "    name = T.name\n",
    "    assert name.endswith('.jpg')\n",
    "    \n",
    "    if name in train:\n",
    "        result_dir = TRAIN_DIR\n",
    "    elif name in val:\n",
    "        result_dir = VAL_DIR\n",
    "    else:\n",
    "        print('WTF')\n",
    "        break\n",
    "    \n",
    "    # crop the image to save space\n",
    "    xmin, ymin, xmax, ymax = boxes[name]\n",
    "    width, height = xmax - xmin, ymax - ymin\n",
    "    assert width > 0 and height > 0\n",
    "    xmin = max(int(xmin - width), 0)\n",
    "    ymin = max(int(ymin - height), 0)\n",
    "    xmax = min(int(xmax + width), w)\n",
    "    ymax = min(int(ymax + height), h)\n",
    "    crop = image[ymin:ymax, xmin:xmax, :]\n",
    "    \n",
    "    # we need to transform annotations after cropping\n",
    "    translation = [xmin, ymin]\n",
    "    \n",
    "    # we need to rename images because of upsampling\n",
    "    new_name = str(counter) + '.jpg'\n",
    "    counter += 1\n",
    "    cv2.imwrite(os.path.join(result_dir, 'images', new_name), crop)\n",
    "\n",
    "    # save annotation for it\n",
    "    d = get_annotation(name, new_name, xmax - xmin, ymax - ymin, translation)\n",
    "    json_name = new_name[:-4] + '.json'\n",
    "    json.dump(d, open(os.path.join(result_dir, 'annotations', json_name), 'w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
